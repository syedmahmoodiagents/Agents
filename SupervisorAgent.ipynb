{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPgrePReKxoqNepVjG3R5hV",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/syedmahmoodiagents/Agents/blob/main/SupervisorAgent.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dDPe8WC1uBwo",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "11d8bfe6-3cf7-445b-e447-29c27c25e569"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/93.8 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m93.8/93.8 kB\u001b[0m \u001b[31m7.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/2.5 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m89.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m81.9/81.9 kB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m156.8/156.8 kB\u001b[0m \u001b[31m12.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m55.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m471.2/471.2 kB\u001b[0m \u001b[31m33.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.2/46.2 kB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.8/56.8 kB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m64.7/64.7 kB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.9/50.9 kB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m208.3/208.3 kB\u001b[0m \u001b[31m15.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "google-colab 1.0.0 requires requests==2.32.4, but you have requests 2.32.5 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "!pip install -U langchain langchain-community langchain-openai langgraph --q"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -U duckduckgo-search --q\n",
        "!pip install -U ddgs --q"
      ],
      "metadata": {
        "id": "lKkTwjr-uQfJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os, getpass"
      ],
      "metadata": {
        "id": "ZjlnbqVyuV9c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "os.environ['OPENAI_API_KEY'] = getpass.getpass()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gyBG104HuZkQ",
        "outputId": "bca99c08-1011-48f4-fa30-da554822f1b5"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "··········\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_openai import ChatOpenAI"
      ],
      "metadata": {
        "id": "6rS5cRqqukx9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "llm = ChatOpenAI(model=\"gpt-4o-mini\")"
      ],
      "metadata": {
        "id": "BqSDDmE7vGtt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# from langchain.agents import create_agent\n",
        "from langgraph.prebuilt import create_react_agent"
      ],
      "metadata": {
        "id": "v7TXKdmvutdd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_community.tools import DuckDuckGoSearchRun, DuckDuckGoSearchResults"
      ],
      "metadata": {
        "id": "16GkaO2_uy9J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "search = DuckDuckGoSearchRun()\n",
        "search.invoke(\"what is AI\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 175
        },
        "id": "nSO0LnuhvDCL",
        "outputId": "b44d20f0-b3fd-4a52-f3a3-d65e0ce43bdf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'1 day ago - It is a field of research in computer science that develops and studies methods and software that enable machines to perceive their environment and use learning and intelligence to take actions that maximize their chances of achieving defined goals. 3 days ago - Artificial general intelligence (AGI)—sometimes called human‑level intelligence AI—is a type of artificial intelligence that would match or surpass human capabilities across virtually all cognitive tasks . March 21, 2025 - What is Artificial intelligence (AI)? AI is the basis for mimicking human intelligence processes through the creation and application of algorithms built into a dynamic computing environment . Read on to learn why AI is important. 2 weeks ago - Artificial intelligence (AI) is technology that enables computers and machines to simulate human learning, comprehension, problem solving, decision-making, creativity and autonomy . Sep 30, 2025 · AI stands for \"artificial intelligence.\" Artificial intelligence is the simulation of human intelligence processes by machines, such as computer systems. AI powers many technology …'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def search_ddgo(query: str):\n",
        "    \"\"\"DuckDuckGo search\"\"\"\n",
        "    search = DuckDuckGoSearchRun()\n",
        "    return search.invoke(query)\n"
      ],
      "metadata": {
        "id": "R4x5guV8vagp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "search_ddgo(\"what are mammals\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 157
        },
        "id": "xe_dgxkQwIis",
        "outputId": "8e91b86c-7b92-484b-8a01-1e2ac76322b1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"Mammal ... A mammal (from Latin mamma 'breast') [1] is a vertebrate animal of the class Mammalia (/ məˈmeɪli.ə /). Mammals are characterised by the presence of milk -producing … Nov 22, 2023 · Mammals are warm-blooded, hair-bearing vertebrates that produce milk. Mammals represent a diverse and fascinating class of animals, encompassing a wide range of … Feb 17, 2025 · Mammals are a group of complex warm-blooded animals belonging to the class Mammalia. They are recognized by the presence of mammary glands (which produce milk to … Apr 29, 2025 · There are eight main characteristics of mammals, ranging from having hair to four-chambered hearts, that set mammals apart from all other vertebrates. All mammals have hair … Mar 16, 2025 · Mammals are characterized by several distinctive features, including a four-chambered heart, a highly developed brain, three middle ear bones, and a specialized …\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def add(a: float,b: float):\n",
        "    \"\"\"adding\"\"\"\n",
        "    return a + b"
      ],
      "metadata": {
        "id": "406PVgHCwNNZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def multiply(a: float,b: float):\n",
        "    \"\"\"multiplying\"\"\"\n",
        "    return a * b"
      ],
      "metadata": {
        "id": "zXHc9WAGwkpQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "math_agent = create_react_agent(\n",
        "    model=llm,\n",
        "    tools=[add, multiply],\n",
        "    name=\"math_agent\",\n",
        "    prompt=\"You are a math agent. Always use one tool at a time\"\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bP6yTJj1wnlZ",
        "outputId": "13c7d4f0-31b5-4d02-eb61-2bdb91b29af4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-2064607087.py:1: LangGraphDeprecatedSinceV10: create_react_agent has been moved to `langchain.agents`. Please update your import to `from langchain.agents import create_agent`. Deprecated in LangGraph V1.0 to be removed in V2.0.\n",
            "  math_agent = create_react_agent(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# mth_agent = create_react_agent(\n",
        "#     model=llm,\n",
        "#     tools=[add, multiply],\n",
        "#     name=\"mth_agent\",\n",
        "#     prompt=\"You are a math agent. Always use one tool at a time\"\n",
        "# )"
      ],
      "metadata": {
        "id": "1yTo4J2cxPOw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "research_agent = create_react_agent(\n",
        "    model=llm,\n",
        "    tools=[search_ddgo],\n",
        "    name=\"res_agent\",\n",
        "    prompt=\"You are a world class reasearcher with access to web search. Do not do any math\"\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nZf2Q6-Syof5",
        "outputId": "0e832e23-7dce-40fd-e22f-0618245ed9a3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-2450890384.py:1: LangGraphDeprecatedSinceV10: create_react_agent has been moved to `langchain.agents`. Please update your import to `from langchain.agents import create_agent`. Deprecated in LangGraph V1.0 to be removed in V2.0.\n",
            "  research_agent = create_react_agent(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Supervisor Agent"
      ],
      "metadata": {
        "id": "SAb8ej-b8pK6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -U langgraph-supervisor --q"
      ],
      "metadata": {
        "id": "kYBx0Xpa0aS_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langgraph_supervisor import create_supervisor"
      ],
      "metadata": {
        "id": "_iL6NBm1zkEO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "workflow = create_supervisor(\n",
        "    [research_agent, math_agent],\n",
        "    model=llm,\n",
        "    prompt=(\n",
        "        \"You are a team supervisor managing a research expert and a math expert\"\n",
        "        \"For current events, use res_agent\"\n",
        "        \"For math agent, use mth_agent\"\n",
        "    )\n",
        ")"
      ],
      "metadata": {
        "id": "pBgBiU_80SAw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "app = workflow.compile()"
      ],
      "metadata": {
        "id": "8-n-EnUd1AIw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "result = app.invoke({\n",
        "    \"messages\": [{\"role\": \"user\", \"content\": \"what is quantum computing ?\"}]\n",
        "})"
      ],
      "metadata": {
        "id": "yPXdfJVx1fgA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "result"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nWrkg09N10eO",
        "outputId": "2ae3fa05-4200-48db-b0a9-d57205897a62"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'messages': [HumanMessage(content='what is quantum computing ?', additional_kwargs={}, response_metadata={}, id='5e103d4f-263b-4d13-8116-6b78f9438941'),\n",
              "  AIMessage(content='Quantum computing is a field of computing that utilizes the principles of quantum mechanics to process information. Unlike classical computers, which use bits to represent data as either 0 or 1, quantum computers employ quantum bits, or qubits. Qubits can exist in multiple states simultaneously due to a property known as superposition. This allows quantum computers to perform many calculations at once.\\n\\nAdditionally, qubits can be entangled, meaning the state of one qubit can be dependent on the state of another, no matter the distance between them. This property enables quantum computers to solve complex problems more efficiently than classical computers in certain cases.\\n\\nQuantum computing holds the potential to revolutionize fields such as cryptography, optimization, material science, and artificial intelligence by tackling problems that are currently intractable with classical computing methods. However, building practical quantum computers is an ongoing area of research with various technical challenges to overcome.', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 181, 'prompt_tokens': 68, 'total_tokens': 249, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_provider': 'openai', 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_560af6e559', 'id': 'chatcmpl-CbC7PZykRktZcAuMEBwzV3jayzfp6', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None}, name='supervisor', id='lc_run--54f17402-be69-414c-a227-30f7ffa49d17-0', usage_metadata={'input_tokens': 68, 'output_tokens': 181, 'total_tokens': 249, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})]}"
            ]
          },
          "metadata": {},
          "execution_count": 49
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "result['messages'][-1].content"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 175
        },
        "id": "_o0GBM7I2aNK",
        "outputId": "e702ad5b-1381-4101-e4f2-9b89fdf9b797"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Quantum computing is a field of computing that utilizes the principles of quantum mechanics to process information. Unlike classical computers, which use bits to represent data as either 0 or 1, quantum computers employ quantum bits, or qubits. Qubits can exist in multiple states simultaneously due to a property known as superposition. This allows quantum computers to perform many calculations at once.\\n\\nAdditionally, qubits can be entangled, meaning the state of one qubit can be dependent on the state of another, no matter the distance between them. This property enables quantum computers to solve complex problems more efficiently than classical computers in certain cases.\\n\\nQuantum computing holds the potential to revolutionize fields such as cryptography, optimization, material science, and artificial intelligence by tackling problems that are currently intractable with classical computing methods. However, building practical quantum computers is an ongoing area of research with various technical challenges to overcome.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 52
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for m in result['messages']:\n",
        "    m.pretty_print()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wQO7bGTd2OMj",
        "outputId": "e6dacc84-7e34-41ef-a8bd-0518c494ad4a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "================================\u001b[1m Human Message \u001b[0m=================================\n",
            "\n",
            "what is quantum computing ?\n",
            "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
            "Name: supervisor\n",
            "\n",
            "Quantum computing is a field of computing that utilizes the principles of quantum mechanics to process information. Unlike classical computers, which use bits to represent data as either 0 or 1, quantum computers employ quantum bits, or qubits. Qubits can exist in multiple states simultaneously due to a property known as superposition. This allows quantum computers to perform many calculations at once.\n",
            "\n",
            "Additionally, qubits can be entangled, meaning the state of one qubit can be dependent on the state of another, no matter the distance between them. This property enables quantum computers to solve complex problems more efficiently than classical computers in certain cases.\n",
            "\n",
            "Quantum computing holds the potential to revolutionize fields such as cryptography, optimization, material science, and artificial intelligence by tackling problems that are currently intractable with classical computing methods. However, building practical quantum computers is an ongoing area of research with various technical challenges to overcome.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "app = workflow.compile()\n",
        "result = app.invoke({\n",
        "    \"messages\": [{\"role\": \"user\", \"content\": \"what is the temperature in Delhi ? Multiply the temperature by 2 and then add 3 to the result\"}]\n",
        "})"
      ],
      "metadata": {
        "id": "lDoTM6gE2WkL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "result"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O2R-E1XF3T3s",
        "outputId": "818793d3-0281-4b53-ffb3-7040c1acd727"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'messages': [HumanMessage(content='what is the temperature in Delhi ? Multiply the temperature by 2 and then add 3 to the result', additional_kwargs={}, response_metadata={}, id='898c0d79-af2c-43b6-bc36-728db0892a80'),\n",
              "  AIMessage(content='', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 12, 'prompt_tokens': 111, 'total_tokens': 123, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_provider': 'openai', 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_560af6e559', 'id': 'chatcmpl-CbCWRSaLNcrv7oKBUrBg2pYjTBwML', 'service_tier': 'default', 'finish_reason': 'tool_calls', 'logprobs': None}, name='supervisor', id='lc_run--b10b564a-6510-4241-942e-f0551df15cad-0', tool_calls=[{'name': 'transfer_to_res_agent', 'args': {}, 'id': 'call_asrgo1uJhc6HGiWE6xIQUMZl', 'type': 'tool_call'}], usage_metadata={'input_tokens': 111, 'output_tokens': 12, 'total_tokens': 123, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}),\n",
              "  ToolMessage(content='Successfully transferred to res_agent', name='transfer_to_res_agent', id='7f90981a-a36f-4d75-99c9-15025178dae2', tool_call_id='call_asrgo1uJhc6HGiWE6xIQUMZl'),\n",
              "  AIMessage(content='The current temperature in Delhi is approximately 13.4°C.', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 14, 'prompt_tokens': 386, 'total_tokens': 400, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_provider': 'openai', 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_560af6e559', 'id': 'chatcmpl-CbCWUM9xWkHrBtw1zyvx8dJEon5oG', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None}, name='res_agent', id='lc_run--fda0c787-3147-4614-a79a-a82d8878274d-0', usage_metadata={'input_tokens': 386, 'output_tokens': 14, 'total_tokens': 400, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}),\n",
              "  AIMessage(content='Transferring back to supervisor', additional_kwargs={}, response_metadata={'__is_handoff_back': True}, name='res_agent', id='7303523a-6691-4abf-9322-7de0a40f04f0', tool_calls=[{'name': 'transfer_back_to_supervisor', 'args': {}, 'id': 'cfa83794-cdb6-4673-966c-1c4a0e00bea1', 'type': 'tool_call'}]),\n",
              "  ToolMessage(content='Successfully transferred back to supervisor', name='transfer_back_to_supervisor', id='a74fafe4-1439-40f2-ad9b-ab116fdcad64', tool_call_id='cfa83794-cdb6-4673-966c-1c4a0e00bea1'),\n",
              "  AIMessage(content='', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 12, 'prompt_tokens': 206, 'total_tokens': 218, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_provider': 'openai', 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_560af6e559', 'id': 'chatcmpl-CbCWVqxo0w4rnzx9NmKmjAclPG8RV', 'service_tier': 'default', 'finish_reason': 'tool_calls', 'logprobs': None}, name='supervisor', id='lc_run--1888aaf8-aa36-4b99-b8bc-aca322e401ce-0', tool_calls=[{'name': 'transfer_to_math_agent', 'args': {}, 'id': 'call_8nY2XLsYhkF3oESCqeZ5vnU1', 'type': 'tool_call'}], usage_metadata={'input_tokens': 206, 'output_tokens': 12, 'total_tokens': 218, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}),\n",
              "  ToolMessage(content='Successfully transferred to math_agent', name='transfer_to_math_agent', id='364b0e3c-6eda-46dd-9930-6c7018db3eae', tool_call_id='call_8nY2XLsYhkF3oESCqeZ5vnU1'),\n",
              "  AIMessage(content='After multiplying the temperature in Delhi (13.4°C) by 2, the result is 26.8. Adding 3 to that gives us a final result of 29.8.', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 41, 'prompt_tokens': 298, 'total_tokens': 339, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_provider': 'openai', 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_560af6e559', 'id': 'chatcmpl-CbCWYEMvmqQfoWW4etCvNSLy0erMu', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None}, name='math_agent', id='lc_run--74caad4e-9851-4e73-a918-51f5e59cbbf8-0', usage_metadata={'input_tokens': 298, 'output_tokens': 41, 'total_tokens': 339, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}),\n",
              "  AIMessage(content='Transferring back to supervisor', additional_kwargs={}, response_metadata={'__is_handoff_back': True}, name='math_agent', id='93220ba1-d5e7-4350-a329-386fff48f59c', tool_calls=[{'name': 'transfer_back_to_supervisor', 'args': {}, 'id': 'cbf973da-729e-4241-b4ca-19dd2d71aa36', 'type': 'tool_call'}]),\n",
              "  ToolMessage(content='Successfully transferred back to supervisor', name='transfer_back_to_supervisor', id='ac499bdb-ebb7-43a7-bcfb-e2ddb613ad75', tool_call_id='cbf973da-729e-4241-b4ca-19dd2d71aa36'),\n",
              "  AIMessage(content='The final result after multiplying the temperature in Delhi (13.4°C) by 2 and then adding 3 is 29.8.', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 30, 'prompt_tokens': 328, 'total_tokens': 358, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_provider': 'openai', 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_560af6e559', 'id': 'chatcmpl-CbCWYI6jXeYMeT6CDPu7JyMp9E2YB', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None}, name='supervisor', id='lc_run--935947a5-99cb-47de-87e8-344b35395c50-0', usage_metadata={'input_tokens': 328, 'output_tokens': 30, 'total_tokens': 358, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})]}"
            ]
          },
          "metadata": {},
          "execution_count": 90
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for m in result['messages']:\n",
        "    m.pretty_print()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DCdY7UyN3a2h",
        "outputId": "77a90b7a-edd0-442f-abaf-6d5fc272ec87"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "================================\u001b[1m Human Message \u001b[0m=================================\n",
            "\n",
            "what is the temperature in Delhi ? Multiply the temperature by 2 and then add 3 to the result\n",
            "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
            "Name: supervisor\n",
            "Tool Calls:\n",
            "  transfer_to_res_agent (call_asrgo1uJhc6HGiWE6xIQUMZl)\n",
            " Call ID: call_asrgo1uJhc6HGiWE6xIQUMZl\n",
            "  Args:\n",
            "=================================\u001b[1m Tool Message \u001b[0m=================================\n",
            "Name: transfer_to_res_agent\n",
            "\n",
            "Successfully transferred to res_agent\n",
            "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
            "Name: res_agent\n",
            "\n",
            "The current temperature in Delhi is approximately 13.4°C.\n",
            "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
            "Name: res_agent\n",
            "\n",
            "Transferring back to supervisor\n",
            "Tool Calls:\n",
            "  transfer_back_to_supervisor (cfa83794-cdb6-4673-966c-1c4a0e00bea1)\n",
            " Call ID: cfa83794-cdb6-4673-966c-1c4a0e00bea1\n",
            "  Args:\n",
            "=================================\u001b[1m Tool Message \u001b[0m=================================\n",
            "Name: transfer_back_to_supervisor\n",
            "\n",
            "Successfully transferred back to supervisor\n",
            "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
            "Name: supervisor\n",
            "Tool Calls:\n",
            "  transfer_to_math_agent (call_8nY2XLsYhkF3oESCqeZ5vnU1)\n",
            " Call ID: call_8nY2XLsYhkF3oESCqeZ5vnU1\n",
            "  Args:\n",
            "=================================\u001b[1m Tool Message \u001b[0m=================================\n",
            "Name: transfer_to_math_agent\n",
            "\n",
            "Successfully transferred to math_agent\n",
            "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
            "Name: math_agent\n",
            "\n",
            "After multiplying the temperature in Delhi (13.4°C) by 2, the result is 26.8. Adding 3 to that gives us a final result of 29.8.\n",
            "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
            "Name: math_agent\n",
            "\n",
            "Transferring back to supervisor\n",
            "Tool Calls:\n",
            "  transfer_back_to_supervisor (cbf973da-729e-4241-b4ca-19dd2d71aa36)\n",
            " Call ID: cbf973da-729e-4241-b4ca-19dd2d71aa36\n",
            "  Args:\n",
            "=================================\u001b[1m Tool Message \u001b[0m=================================\n",
            "Name: transfer_back_to_supervisor\n",
            "\n",
            "Successfully transferred back to supervisor\n",
            "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
            "Name: supervisor\n",
            "\n",
            "The final result after multiplying the temperature in Delhi (13.4°C) by 2 and then adding 3 is 29.8.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# def supervisor(state: AgentState):\n",
        "#     last = state[\"messages\"][-1][\"content\"]\n",
        "\n",
        "#     # Very simple routing logic\n",
        "#     if any(c.isdigit() for c in last):\n",
        "#         selected = \"math_agent\"\n",
        "#     else:\n",
        "#         selected = \"search_agent\"\n",
        "\n",
        "#     return {\"selected_agent\": selected}\n"
      ],
      "metadata": {
        "id": "gJvfcEqQ4UUn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# !pip install -U langchain langchain-community langchain-openai langgraph langchain-core==1.0.3 --quiet"
      ],
      "metadata": {
        "id": "M3o32yrLE0nP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import TypedDict, Annotated, List, Union\n",
        "from langchain_core.agents import AgentFinish\n",
        "from langgraph.graph import StateGraph, END\n",
        "from langchain_core.messages import BaseMessage, HumanMessage, AIMessage, FunctionMessage\n",
        "import operator\n",
        "\n",
        "\n",
        "# Define the graph state\n",
        "class AgentState(TypedDict):\n",
        "    messages: Annotated[List[BaseMessage], operator.add]\n",
        "    current_agent: str\n",
        "\n",
        "\n",
        "# Define the agent nodes\n",
        "def agent_node(state, agent, name):\n",
        "    result = agent.invoke(state[\"messages\"])\n",
        "    if isinstance(result, AgentFinish):\n",
        "        return {\n",
        "            \"messages\": [AIMessage(content=result.return_values[\"output\"], name=name)],\n",
        "            \"current_agent\": \"supervisor\",\n",
        "        }\n",
        "    else:\n",
        "        return {\n",
        "            \"messages\": [AIMessage(content=str(result), name=name)],\n",
        "            \"current_agent\": \"supervisor\",\n",
        "        }\n",
        "\n",
        "\n",
        "# Supervisor node (router)\n",
        "def supervisor_node(state: AgentState):\n",
        "    last_message = state[\"messages\"][-1]\n",
        "\n",
        "    # If the last message is from a tool, try to route back to supervisor to decide next step\n",
        "    if isinstance(last_message, FunctionMessage):\n",
        "        return \"supervisor\"\n",
        "\n",
        "    # Decide which agent to call next based on the user's intent\n",
        "    # For this example, we'll keep it simple and directly route based on keywords.\n",
        "    # In a real scenario, you'd use a more sophisticated LLM-based router.\n",
        "    user_message_content = last_message.content.lower()\n",
        "\n",
        "    if any(keyword in user_message_content for keyword in [\"multiply\", \"add\", \"math\"]):\n",
        "        return \"math_agent\"\n",
        "    elif any(keyword in user_message_content for keyword in [\"temperature\", \"what is\", \"research\"]):\n",
        "        return \"res_agent\"\n",
        "    else:\n",
        "        # Default to research agent if no specific keyword is found\n",
        "        return \"res_agent\"\n",
        "\n",
        "\n",
        "# Build the StateGraph\n",
        "workflow = StateGraph(AgentState)\n",
        "\n",
        "# Add nodes for each agent\n",
        "workflow.add_node(\"res_agent\", lambda state: agent_node(state, research_agent, \"res_agent\"))\n",
        "workflow.add_node(\"math_agent\", lambda state: agent_node(state, math_agent, \"math_agent\"))\n",
        "workflow.add_node(\"supervisor\", supervisor_node)\n",
        "\n",
        "# Set up the entry point: supervisor starts the conversation\n",
        "workflow.set_entry_point(\"supervisor\")\n",
        "\n",
        "# Add edges: from supervisor to agents based on routing\n",
        "workflow.add_edge(\"supervisor\", \"res_agent\")\n",
        "workflow.add_edge(\"supervisor\", \"math_agent\")\n",
        "\n",
        "# Add edges: from agents back to supervisor, or to END if the agent has finished its task\n",
        "workflow.add_edge(\"res_agent\", \"supervisor\")\n",
        "workflow.add_edge(\"math_agent\", \"supervisor\")\n",
        "\n",
        "# This simplified example doesn't have an explicit 'END' condition for the supervisor to decide it's done\n",
        "# In a more complex scenario, the supervisor would have a tool to signal completion.\n",
        "# For now, let's assume the supervisor will eventually respond directly if it doesn't route.\n",
        "\n",
        "app = workflow.compile()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 633
        },
        "id": "XFJeK7vJAI4S",
        "outputId": "6f8e04b2-e7d8-4ba0-e664-981b9b9812dd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[33mWARNING: Skipping langchain as it is not installed.\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mWARNING: Skipping langchain-community as it is not installed.\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mWARNING: Skipping langchain-openai as it is not installed.\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mWARNING: Skipping langgraph as it is not installed.\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mWARNING: Skipping langchain-core as it is not installed.\u001b[0m\u001b[33m\n",
            "\u001b[0m"
          ]
        },
        {
          "output_type": "error",
          "ename": "ImportError",
          "evalue": "cannot import name '_extract_reasoning_from_additional_kwargs' from 'langchain_core.messages.base' (/usr/local/lib/python3.12/dist-packages/langchain_core/messages/base.py)",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-2027571282.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtyping\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mTypedDict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mAnnotated\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mList\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mUnion\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mlangchain_core\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0magents\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mAgentFinish\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mlanggraph\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgraph\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mStateGraph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mEND\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mlangchain_core\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessages\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mBaseMessage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mHumanMessage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mAIMessage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mFunctionMessage\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0moperator\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/langgraph/graph/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mlanggraph\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconstants\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mEND\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mSTART\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mlanggraph\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgraph\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mMessageGraph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mMessagesState\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0madd_messages\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mlanggraph\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgraph\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mStateGraph\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m __all__ = (\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/langgraph/graph/message.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     12\u001b[0m )\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m from langchain_core.messages import (\n\u001b[0m\u001b[1;32m     15\u001b[0m     \u001b[0mAnyMessage\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0mBaseMessage\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/langchain_core/messages/__init__.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(attr_name)\u001b[0m\n\u001b[1;32m    142\u001b[0m     \u001b[0;34m\"Citation\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m\"content\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    143\u001b[0m     \u001b[0;34m\"ContentBlock\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m\"content\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 144\u001b[0;31m     \u001b[0;34m\"ChatMessage\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m\"chat\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    145\u001b[0m     \u001b[0;34m\"ChatMessageChunk\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m\"chat\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    146\u001b[0m     \u001b[0;34m\"DataContentBlock\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m\"content\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/langchain_core/_import_utils.py\u001b[0m in \u001b[0;36mimport_attr\u001b[0;34m(attr_name, module_name, package)\u001b[0m\n\u001b[1;32m     34\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m             \u001b[0mmodule\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimport_module\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\".{module_name}\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpackage\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpackage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 36\u001b[0;31m         \u001b[0;32mexcept\u001b[0m \u001b[0mModuleNotFoundError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     37\u001b[0m             \u001b[0mmsg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34mf\"module '{package!r}.{module_name!r}' not found ({err})\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mImportError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.12/importlib/__init__.py\u001b[0m in \u001b[0;36mimport_module\u001b[0;34m(name, package)\u001b[0m\n\u001b[1;32m     88\u001b[0m                 \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m             \u001b[0mlevel\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 90\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_bootstrap\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_gcd_import\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlevel\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpackage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlevel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     91\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/langchain_core/messages/utils.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mlangchain_core\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessages\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mai\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mAIMessage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mAIMessageChunk\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mlangchain_core\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessages\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbase\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mBaseMessage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mBaseMessageChunk\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m from langchain_core.messages.block_translators.openai import (\n\u001b[0m\u001b[1;32m     34\u001b[0m     \u001b[0mconvert_to_openai_data_block\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m )\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/langchain_core/messages/block_translators/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    109\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 111\u001b[0;31m \u001b[0m_register_translators\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/langchain_core/messages/block_translators/__init__.py\u001b[0m in \u001b[0;36m_register_translators\u001b[0;34m()\u001b[0m\n\u001b[1;32m     93\u001b[0m         \u001b[0m_register_google_vertexai_translator\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m     )\n\u001b[0;32m---> 95\u001b[0;31m     from langchain_core.messages.block_translators.groq import (  # noqa: PLC0415\n\u001b[0m\u001b[1;32m     96\u001b[0m         \u001b[0m_register_groq_translator\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m     )\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/langchain_core/messages/block_translators/groq.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mlangchain_core\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessages\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mAIMessage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mAIMessageChunk\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mlangchain_core\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessages\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcontent\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtypes\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mlangchain_core\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessages\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbase\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0m_extract_reasoning_from_additional_kwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mImportError\u001b[0m: cannot import name '_extract_reasoning_from_additional_kwargs' from 'langchain_core.messages.base' (/usr/local/lib/python3.12/dist-packages/langchain_core/messages/base.py)",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ]
    }
  ]
}