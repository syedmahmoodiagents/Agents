{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMGcmeVBCfUSzStD/TG4olh",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/syedmahmoodiagents/Agents/blob/main/RAG_LangChain.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install faiss-cpu --q"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e5d9HN-v9caN",
        "outputId": "9346c337-1690-4c0a-ebb1-f55ad80a6413"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m23.6/23.6 MB\u001b[0m \u001b[31m22.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -qU langchain langchain-community langchain-text-splitters --q"
      ],
      "metadata": {
        "id": "ivHFtqa2VRLs",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4ab65ec3-7a02-453d-94d3-26965efb6143"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/2.5 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m78.5 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m46.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/1.0 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m52.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/64.7 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m64.7/64.7 kB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/50.9 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.9/50.9 kB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "google-colab 1.0.0 requires requests==2.32.4, but you have requests 2.32.5 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -qU langchain-openai langchain-faiss --q"
      ],
      "metadata": {
        "id": "DsUDrVZg0Noj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "17f8312b-76a4-4ff7-8389-e255b7fc5d05"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/84.3 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━\u001b[0m \u001b[32m81.9/84.3 kB\u001b[0m \u001b[31m8.9 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.3/84.3 kB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "from langchain_core.messages import HumanMessage, AIMessage"
      ],
      "metadata": {
        "id": "1e-09v3mzxc-"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.runnables import RunnableWithMessageHistory\n",
        "from langchain_core.chat_history import BaseChatMessageHistory, InMemoryChatMessageHistory"
      ],
      "metadata": {
        "id": "wKsULGYGNNU6"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
        "from langchain_community.vectorstores import FAISS\n",
        "from langchain_openai import OpenAIEmbeddings, ChatOpenAI\n",
        "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder"
      ],
      "metadata": {
        "id": "A5ZWz0-t8y3d"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os, getpass"
      ],
      "metadata": {
        "id": "cBEFrtKy7M5w"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "os.environ[\"OPENAI_API_KEY\"] = getpass.getpass() # can be replace by Ollama settings"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v9CUiz-g7Psl",
        "outputId": "06e8e3c4-72b8-4287-c9b7-b53f14699ea6"
      },
      "execution_count": 8,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "··········\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "llm = ChatOpenAI(model=\"gpt-4o-mini\")"
      ],
      "metadata": {
        "id": "cViwGWVE7aaP"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "texts = [\n",
        "    \"LangChain helps developers build LLM applications.\",\n",
        "    \"FAISS is used for vector similarity search.\",\n",
        "    \"Chat history must be manually maintained in LangChain 1.1.\",\n",
        "    \"Retrievers are used in RAG pipelines.\",\n",
        "    \"OpenAI embeddings create vector representations.\"\n",
        "]"
      ],
      "metadata": {
        "id": "PgBj1h-CKmTW"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "embeddings = OpenAIEmbeddings()"
      ],
      "metadata": {
        "id": "HJpGnw6TKqa_"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "db = FAISS.from_texts(texts, embeddings)"
      ],
      "metadata": {
        "id": "bjZsp_zdK4GJ"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "retriever = db.as_retriever()"
      ],
      "metadata": {
        "id": "2Y2er-n5K52T"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = ChatPromptTemplate.from_messages([\n",
        "    (\"system\", \"Use the retrieved context to answer the user.\"),\n",
        "    (\"human\", \"{question}\\n\\nContext:\\n{context}\")\n",
        "])"
      ],
      "metadata": {
        "id": "Rsgr-jtVK7qD"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "chain = prompt | llm"
      ],
      "metadata": {
        "id": "ibe5_d77K9gW"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "history = []   # store HumanMessage + AIMessage objects"
      ],
      "metadata": {
        "id": "WADx97nlK_LK"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def ask(question):\n",
        "    global history\n",
        "\n",
        "    # Retrieve context documents\n",
        "    docs = retriever.invoke(question)\n",
        "    context = \"\\n\".join([d.page_content for d in docs])\n",
        "\n",
        "    # Add human message to history\n",
        "    history.append(HumanMessage(content=question))\n",
        "\n",
        "    # Run the RAG chain\n",
        "    response = chain.invoke({\n",
        "        \"context\": context,\n",
        "        \"messages\": history,\n",
        "        \"question\": question\n",
        "    })\n",
        "\n",
        "    # Add AI reply to history\n",
        "    history.append(AIMessage(content=response.content))\n",
        "\n",
        "    return response.content"
      ],
      "metadata": {
        "id": "XBG-ALevLBkm"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"User: What is FAISS?\")\n",
        "print(\"AI:\", ask(\"What is FAISS?\"))\n",
        "\n",
        "print(\"\\nUser: What did I ask earlier?\")\n",
        "print(\"AI:\", ask(\"What did I ask earlier?\"))\n",
        "\n",
        "print(\"\\nUser: How does LangChain handle memory?\")\n",
        "print(\"AI:\", ask(\"How does LangChain handle memory?\"))\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QFrrLFEWKFWi",
        "outputId": "5a4e2a3f-19e5-4a09-bb1e-e02a695b7d97"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "User: What is FAISS?\n",
            "AI: FAISS, which stands for Facebook AI Similarity Search, is a library designed for efficient vector similarity search. It is widely used to find similar items or perform nearest neighbor searches within large datasets of vector embeddings. This is particularly useful in applications like recommender systems or search features, where embeddings represent items, texts, or images in a high-dimensional space. In the context you provided, FAISS can be employed with OpenAI embeddings to perform similarity searches in retrieval-augmented generation (RAG) pipelines, enhancing the ability to fetch relevant information based on vector representations.\n",
            "\n",
            "User: What did I ask earlier?\n",
            "AI: I'm sorry, but I don't have access to previous interactions or the specific questions you may have asked earlier. How can I assist you today?\n",
            "\n",
            "User: How does LangChain handle memory?\n",
            "AI: LangChain handles memory by requiring developers to manually maintain chat history in version 1.1. This means that you'd need to implement your own system to keep track of the interactions within your application. Additionally, you can use retrievers in Retrieval-Augmented Generation (RAG) pipelines to help manage and retrieve relevant contextual information, though this does not directly pertain to memory but can assist in preserving context. Furthermore, OpenAI embeddings aid in creating vector representations that can be useful for similarity searches or other memory-related operations.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# One more example"
      ],
      "metadata": {
        "id": "I1dL6072KtOs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "docs = [\"LangChain 1.x has changed imports. ConversationalRetrievalChain is deprecated.\"]"
      ],
      "metadata": {
        "id": "US2XRkFw96Ym"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "splitter = RecursiveCharacterTextSplitter(chunk_size=200, chunk_overlap=50)\n",
        "chunks = splitter.create_documents(docs)"
      ],
      "metadata": {
        "id": "PkN5rRLT98l_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "emb = OpenAIEmbeddings()\n",
        "vectordb = FAISS.from_documents(chunks, emb)"
      ],
      "metadata": {
        "id": "xH7VpK-n9_c_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "retriever = vectordb.as_retriever()"
      ],
      "metadata": {
        "id": "rLZgPjDQ-B_R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = ChatPromptTemplate.from_template(\"\"\"\n",
        "You are an assistant that answers based on retrieved context.\n",
        "\n",
        "Context:\n",
        "{context}\n",
        "\n",
        "Question:\n",
        "{question}\n",
        "\n",
        "Answer:\n",
        "\"\"\")"
      ],
      "metadata": {
        "id": "uaMH-xj0-JvY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_answer(query):\n",
        "    context_docs = retriever.invoke(query)\n",
        "    context = \"\\n\\n\".join([d.page_content for d in context_docs])\n",
        "\n",
        "    chain = prompt | llm\n",
        "    return chain.invoke({\"context\": context, \"question\": query})"
      ],
      "metadata": {
        "id": "tVlxZHV9-NM_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(get_answer(\"What happened to ConversationalRetrievalChain?\"))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_n7YzwMH8w1H",
        "outputId": "7c52a1ad-1da8-40a0-8b79-b705d73662ce"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "content=\"In LangChain 1.x, the `ConversationalRetrievalChain` has been deprecated. This likely means that the functionality provided by `ConversationalRetrievalChain` has either been replaced by new components or incorporated into a different module or class structure. It's common for libraries to deprecate certain components as they evolve and improve their architecture to offer better modularity, performance, or usability. Users are typically encouraged to use the updated or new components as specified in the latest version of the library's documentation.\" additional_kwargs={'refusal': None} response_metadata={'token_usage': {'completion_tokens': 102, 'prompt_tokens': 51, 'total_tokens': 153, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_provider': 'openai', 'model_name': 'gpt-4o-2024-08-06', 'system_fingerprint': 'fp_e819e3438b', 'id': 'chatcmpl-CiNp6oertpaXc1Z8OJpKcSaVPXRFV', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None} id='lc_run--42e76d3d-c36d-4886-bfeb-de732e36879c-0' usage_metadata={'input_tokens': 51, 'output_tokens': 102, 'total_tokens': 153, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "IRx2FxhBJUE1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Using fromMessages"
      ],
      "metadata": {
        "id": "aXvFwJx49yGN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = ChatPromptTemplate.from_messages([\n",
        "    (\"system\", \"You are a helpful assistant.\"),\n",
        "    (\"human\", \"{question}\"),\n",
        "    (\"human\", \"Relevant documents: {context}\"),\n",
        "])"
      ],
      "metadata": {
        "id": "0PL2JMwO7czQ"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def format_docs(docs):\n",
        "    return \"\\n\\n\".join(doc.page_content for doc in docs)"
      ],
      "metadata": {
        "id": "ZMDm913A7ibX"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def conversational_rag(question, chat_history):\n",
        "    docs = retriever.get_relevant_documents(question)\n",
        "    context = format_docs(docs)\n",
        "\n",
        "    messages = [\n",
        "        *chat_history,\n",
        "        HumanMessage(content=prompt.format(question=question, context=context).to_string())\n",
        "    ]\n",
        "\n",
        "    response = llm(messages)\n",
        "    return response.content\n"
      ],
      "metadata": {
        "id": "yf63KpVGzuMg"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def get_answer(query):\n",
        "    context_docs = retriever.invoke(query)\n",
        "    context = \"\\n\\n\".join([d.page_content for d in context_docs])\n",
        "\n",
        "    chain = prompt | llm\n",
        "    return chain.invoke({\"context\": context, \"question\": query})\n"
      ],
      "metadata": {
        "id": "yk56OVEK8LHQ"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(get_answer(\"What happened to ConversationalRetrievalChain?\"))"
      ],
      "metadata": {
        "id": "l7aS8sGY8Xgr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "60981166-5ff6-4af8-af0c-bdd98cebd17e"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "content='ConversationalRetrievalChain is part of LangChain, which is a framework designed to help developers create applications using large language models (LLMs). In the context of LangChain, the ConversationalRetrievalChain typically enables the ability to perform retrieval-augmented generation (RAG) by allowing models to access relevant documents based on user queries and incorporate this information into the conversation flow.\\n\\nAs for the specifics of what happened to ConversationalRetrievalChain, it likely refers to changes or updates made to the functionality or implementation within LangChain, especially if there was a newer version released after 1.1. In this version, manual chat history maintenance is required, indicating that developers need to manage the context and previous interactions themselves. \\n\\nIf you are looking for further details on specific updates or features introduced after LangChain 1.1, checking the LangChain documentation or release notes would provide the most accurate and recent information.' additional_kwargs={'refusal': None} response_metadata={'token_usage': {'completion_tokens': 185, 'prompt_tokens': 72, 'total_tokens': 257, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_provider': 'openai', 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_50906f2aac', 'id': 'chatcmpl-CigRrurCBZyKxXFKjIpaZj1od6TLu', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None} id='lc_run--0cda9719-597d-4c49-bf13-b0f0c4a0914f-0' usage_metadata={'input_tokens': 72, 'output_tokens': 185, 'total_tokens': 257, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Igl_plKnLVyi"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}