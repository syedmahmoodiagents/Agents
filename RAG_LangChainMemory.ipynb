{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyP5Jm69o2ZRFeOGuH0pF2su",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/syedmahmoodiagents/Agents/blob/main/RAG_LangChainMemory.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install faiss-cpu --q"
      ],
      "metadata": {
        "id": "e5d9HN-v9caN"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -qU langchain langchain-community langchain-text-splitters --q"
      ],
      "metadata": {
        "id": "ivHFtqa2VRLs"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -qU langchain-openai langchain-faiss --q"
      ],
      "metadata": {
        "id": "DsUDrVZg0Noj"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
        "from langchain_core.messages import HumanMessage, AIMessage"
      ],
      "metadata": {
        "id": "1e-09v3mzxc-"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.runnables import RunnableWithMessageHistory\n",
        "from langchain_core.chat_history import BaseChatMessageHistory, InMemoryChatMessageHistory"
      ],
      "metadata": {
        "id": "wKsULGYGNNU6"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
        "from langchain_community.vectorstores import FAISS\n",
        "from langchain_openai import OpenAIEmbeddings, ChatOpenAI"
      ],
      "metadata": {
        "id": "A5ZWz0-t8y3d"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.runnables import RunnablePassthrough"
      ],
      "metadata": {
        "id": "_aGzDvI_aihL"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os, getpass"
      ],
      "metadata": {
        "id": "cBEFrtKy7M5w"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "os.environ[\"OPENAI_API_KEY\"] = getpass.getpass() # can be replace by Ollama settings"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v9CUiz-g7Psl",
        "outputId": "f6e23926-4916-4075-b79c-76f4ae03cf54"
      },
      "execution_count": 9,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "··········\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "llm = ChatOpenAI(model=\"gpt-4o-mini\")"
      ],
      "metadata": {
        "id": "cViwGWVE7aaP"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "texts = [\n",
        "    \"LangChain helps developers build LLM applications.\",\n",
        "    \"FAISS is used for vector similarity search.\",\n",
        "    \"Chat history must be manually maintained in LangChain 1.1.\",\n",
        "    \"Retrievers are used in RAG pipelines.\",\n",
        "    \"OpenAI embeddings create vector representations.\"\n",
        "]"
      ],
      "metadata": {
        "id": "PgBj1h-CKmTW"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "embeddings = OpenAIEmbeddings()"
      ],
      "metadata": {
        "id": "HJpGnw6TKqa_"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "db = FAISS.from_texts(texts, embeddings)"
      ],
      "metadata": {
        "id": "bjZsp_zdK4GJ"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "retriever = db.as_retriever()"
      ],
      "metadata": {
        "id": "2Y2er-n5K52T"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "store = {}\n",
        "def get_session_history(session_id: str) -> BaseChatMessageHistory:\n",
        "    if session_id not in store:\n",
        "        store[session_id] = InMemoryChatMessageHistory()\n",
        "    return store[session_id]"
      ],
      "metadata": {
        "id": "6d4PjCUDcZgp"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "rag_prompt_with_history = ChatPromptTemplate.from_messages([\n",
        "    (\"system\", \"Use the retrieved context to answer the user.\"),\n",
        "    MessagesPlaceholder(\"history\"), # This is where the history will be injected\n",
        "    (\"human\", \"{question}\\n\\nContext:\\n{context}\")\n",
        "])"
      ],
      "metadata": {
        "id": "by2ZAgDzcc0l"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_context_from_retriever(question_dict):\n",
        "    # The input to this function will be a dictionary, e.g., {'question': '...'}\n",
        "    docs = retriever.invoke(question_dict[\"question\"])\n",
        "    return \"\\n\".join([d.page_content for d in docs])"
      ],
      "metadata": {
        "id": "o71IVlr6cfV2"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "rag_chain_with_context = (\n",
        "    RunnablePassthrough.assign(context=get_context_from_retriever)\n",
        "    | rag_prompt_with_history\n",
        "    | llm\n",
        ")"
      ],
      "metadata": {
        "id": "kXVYi0ZHclFK"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "conversational_rag_chain_with_history = RunnableWithMessageHistory(\n",
        "    rag_chain_with_context,\n",
        "    get_session_history,\n",
        "    input_messages_key=\"question\", # Key in the input dict for the user's question\n",
        "    history_messages_key=\"history\", # Key in the prompt for the chat history\n",
        ")"
      ],
      "metadata": {
        "id": "qdlDmrE1cpcs"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def ask_with_managed_history(question: str, session_id: str = \"default_session\"):\n",
        "    response = conversational_rag_chain_with_history.invoke(\n",
        "        {\"question\": question}, # Input only needs the question now\n",
        "        config={\n",
        "            \"configurable\": {\"session_id\": session_id}\n",
        "        }\n",
        "    )\n",
        "    return response.content"
      ],
      "metadata": {
        "id": "-qTjnXqhcuG1"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "store.clear()"
      ],
      "metadata": {
        "id": "6mGoQ2lecxob"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"User (session1): What is FAISS?\")\n",
        "print(\"AI (session1):\", ask_with_managed_history(\"What is FAISS?\", session_id=\"session1\"))\n",
        "\n",
        "print(\"\\nUser (session1): What did I ask earlier?\")\n",
        "print(\"AI (session1):\", ask_with_managed_history(\"What did I ask earlier?\", session_id=\"session1\"))\n",
        "\n",
        "print(\"\\nUser (session2): How does LangChain handle memory?\")\n",
        "print(\"AI (session2):\", ask_with_managed_history(\"How does LangChain handle memory?\", session_id=\"session2\"))\n",
        "\n",
        "print(\"\\nUser (session1): And what about LangChain's memory?\")\n",
        "print(\"AI (session1):\", ask_with_managed_history(\"And what about LangChain's memory?\", session_id=\"session1\"))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8q6q-bsMZN4r",
        "outputId": "fab9d4ac-272e-407b-bb0f-6b3e35bb93b0"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "User (session1): What is FAISS?\n",
            "AI (session1): FAISS, which stands for Facebook AI Similarity Search, is a library designed for efficient similarity search and clustering of dense vectors. It is particularly useful in scenarios where you need to find similar items based on their vector representations, making it ideal for applications such as information retrieval and recommendation systems. In the context of OpenAI embeddings, FAISS can be used to perform vector similarity searches on the embeddings generated, facilitating the retrieval of relevant information in systems like retrieval-augmented generation (RAG) pipelines, where retrievers, often implemented with tools like LangChain, help in building applications that utilize large language models (LLMs).\n",
            "\n",
            "User (session1): What did I ask earlier?\n",
            "AI (session1): You asked about FAISS.\n",
            "\n",
            "User (session2): How does LangChain handle memory?\n",
            "AI (session2): In LangChain, memory management involves manually maintaining chat history, especially in version 1.1. This means that developers need to track the conversation context themselves to ensure continuity in LLM applications, as there isn't an automatic memory retention feature. Additionally, in RAG (Retrieval-Augmented Generation) pipelines, retrievers play a crucial role in fetching relevant information based on the context of the ongoing interaction. Overall, managing memory in LangChain requires developers to implement systems to store and manage chat history effectively.\n",
            "\n",
            "User (session1): And what about LangChain's memory?\n",
            "AI (session1): In LangChain, memory refers to the capability to maintain context or store past interactions in a conversation. As of LangChain version 1.1, chat history must be manually maintained, meaning that developers need to handle the storage and retrieval of previous messages themselves to provide continuity in interactions. This is important for applications that rely on large language models (LLMs) to generate coherent and contextually relevant responses based on prior exchanges in a conversation. Memory can help create more engaging and personalized user experiences by allowing the model to remember details across interactions.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# 1. Define a store for chat session\n",
        "\n",
        "# 2. New prompt with history placeholder\n",
        "# This prompt now explicitly expects the chat history as part of its messages.\n",
        "\n",
        "# 3. Define a processing step to get context from the retriever\n",
        "\n",
        "# 4. Create the RAG chain with context retrieval and history handling\n",
        "# RunnablePassthrough.assign is used to add 'context' to the input dictionary\n",
        "# before passing it to the prompt.\n",
        "\n",
        "# 5. Wrap this chain with RunnableWithMessageHistory\n",
        "# This runnable automatically manages adding messages to history\n",
        "# and retrieving them based on the session_id.\n",
        "\n",
        "# Example usage function with managed history\n"
      ],
      "metadata": {
        "id": "BEMRgewIcydS"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}