{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMzm7nC9gWh1gG0pRdmuA4l",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/syedmahmoodiagents/Agents/blob/main/RAG_LangChainMemory.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install faiss-cpu --q"
      ],
      "metadata": {
        "id": "e5d9HN-v9caN"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -qU langchain langchain-community langchain-text-splitters --q"
      ],
      "metadata": {
        "id": "ivHFtqa2VRLs"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -qU langchain-openai langchain-faiss --q"
      ],
      "metadata": {
        "id": "DsUDrVZg0Noj"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
        "from langchain_core.messages import HumanMessage, AIMessage"
      ],
      "metadata": {
        "id": "1e-09v3mzxc-"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.runnables import RunnableWithMessageHistory\n",
        "from langchain_core.chat_history import BaseChatMessageHistory, InMemoryChatMessageHistory"
      ],
      "metadata": {
        "id": "wKsULGYGNNU6"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
        "from langchain_community.vectorstores import FAISS\n",
        "from langchain_openai import OpenAIEmbeddings, ChatOpenAI"
      ],
      "metadata": {
        "id": "A5ZWz0-t8y3d"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.runnables import RunnablePassthrough"
      ],
      "metadata": {
        "id": "_aGzDvI_aihL"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os, getpass"
      ],
      "metadata": {
        "id": "cBEFrtKy7M5w"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "os.environ[\"OPENAI_API_KEY\"] = getpass.getpass() # can be replace by Ollama settings"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v9CUiz-g7Psl",
        "outputId": "f6e23926-4916-4075-b79c-76f4ae03cf54"
      },
      "execution_count": 9,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "··········\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "llm = ChatOpenAI(model=\"gpt-4o-mini\")"
      ],
      "metadata": {
        "id": "cViwGWVE7aaP"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "texts = [\n",
        "    \"LangChain helps developers build LLM applications.\",\n",
        "    \"FAISS is used for vector similarity search.\",\n",
        "    \"Chat history must be manually maintained in LangChain 1.1.\",\n",
        "    \"Retrievers are used in RAG pipelines.\",\n",
        "    \"OpenAI embeddings create vector representations.\"\n",
        "]"
      ],
      "metadata": {
        "id": "PgBj1h-CKmTW"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "embeddings = OpenAIEmbeddings()"
      ],
      "metadata": {
        "id": "HJpGnw6TKqa_"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "db = FAISS.from_texts(texts, embeddings)"
      ],
      "metadata": {
        "id": "bjZsp_zdK4GJ"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "retriever = db.as_retriever()"
      ],
      "metadata": {
        "id": "2Y2er-n5K52T"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "store = {}\n",
        "def get_session_history(session_id: str) -> BaseChatMessageHistory:\n",
        "    if session_id not in store:\n",
        "        store[session_id] = InMemoryChatMessageHistory()\n",
        "    return store[session_id]"
      ],
      "metadata": {
        "id": "6d4PjCUDcZgp"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "rag_prompt_with_history = ChatPromptTemplate.from_messages([\n",
        "    (\"system\", \"Use the retrieved context to answer the user.\"),\n",
        "    MessagesPlaceholder(\"history\"), # This is where the history will be injected\n",
        "    (\"human\", \"{question}\\n\\nContext:\\n{context}\")\n",
        "])"
      ],
      "metadata": {
        "id": "by2ZAgDzcc0l"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_context_from_retriever(question_dict):\n",
        "    # The input to this function will be a dictionary, e.g., {'question': '...'}\n",
        "    docs = retriever.invoke(question_dict[\"question\"])\n",
        "    return \"\\n\".join([d.page_content for d in docs])"
      ],
      "metadata": {
        "id": "o71IVlr6cfV2"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "runnable = RunnablePassthrough.assign(context=get_context_from_retriever)"
      ],
      "metadata": {
        "id": "frOxfv8Xfzsc"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "rag_chain_with_context = (\n",
        "    runnable | rag_prompt_with_history | llm\n",
        ")"
      ],
      "metadata": {
        "id": "kXVYi0ZHclFK"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "conversational_rag_chain_with_history = RunnableWithMessageHistory(\n",
        "    rag_chain_with_context,\n",
        "    get_session_history,\n",
        "    input_messages_key=\"question\", # Key in the input dict for the user's question\n",
        "    history_messages_key=\"history\", # Key in the prompt for the chat history\n",
        ")"
      ],
      "metadata": {
        "id": "qdlDmrE1cpcs"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def ask_with_managed_history(question: str, session_id: str = \"default_session\"):\n",
        "    response = conversational_rag_chain_with_history.invoke(\n",
        "        {\"question\": question}, # Input only needs the question now\n",
        "        config={\n",
        "            \"configurable\": {\"session_id\": session_id}\n",
        "        }\n",
        "    )\n",
        "    return response.content"
      ],
      "metadata": {
        "id": "-qTjnXqhcuG1"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "store.clear()"
      ],
      "metadata": {
        "id": "6mGoQ2lecxob"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"User (session1): What is FAISS?\")\n",
        "print(\"AI (session1):\", ask_with_managed_history(\"What is FAISS?\", session_id=\"session1\"))\n",
        "\n",
        "print(\"\\nUser (session1): What did I ask earlier?\")\n",
        "print(\"AI (session1):\", ask_with_managed_history(\"What did I ask earlier?\", session_id=\"session1\"))\n",
        "\n",
        "print(\"\\nUser (session2): How does LangChain handle memory?\")\n",
        "print(\"AI (session2):\", ask_with_managed_history(\"How does LangChain handle memory?\", session_id=\"session2\"))\n",
        "\n",
        "print(\"\\nUser (session1): And what about LangChain's memory?\")\n",
        "print(\"AI (session1):\", ask_with_managed_history(\"And what about LangChain's memory?\", session_id=\"session1\"))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8q6q-bsMZN4r",
        "outputId": "55e28a41-59eb-433a-fcf1-c064d070ee9d"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "User (session1): What is FAISS?\n",
            "AI (session1): FAISS (Facebook AI Similarity Search) is a library designed for efficient similarity search and clustering of dense vectors. It is particularly useful for tasks such as vector similarity search, enabling applications to quickly find similar items based on their vector representations. When working with OpenAI embeddings, which create these vector representations, FAISS can be used to effectively retrieve relevant data in scenarios such as retrieval-augmented generation (RAG) pipelines. Additionally, libraries like LangChain assist developers in building applications leveraging large language models (LLMs), often utilizing FAISS for managing vector searches.\n",
            "\n",
            "User (session1): What did I ask earlier?\n",
            "AI (session1): You asked about FAISS.\n",
            "\n",
            "User (session2): How does LangChain handle memory?\n",
            "AI (session2): In LangChain, memory management is primarily a manual process, particularly in version 1.1 where chat history must be explicitly maintained by developers. This means that you need to design your application to keep track of the conversation history or relevant context as needed. \n",
            "\n",
            "Additionally, LangChain supports retrieval-augmented generation (RAG) pipelines where retrievers can be employed to fetch pertinent information during interactions. To enhance context understanding and response accuracy, OpenAI embeddings can be used to create vector representations of inputs, helping to bridge the memory between user inputs and relevant contextual data.\n",
            "\n",
            "In summary, while LangChain provides tools for integrating memory through chat history management and contextual retrieval, it relies on developers to handle the specifics of memory implementation in their applications.\n",
            "\n",
            "User (session1): And what about LangChain's memory?\n",
            "AI (session1): LangChain's memory refers to the ability to maintain chat history and contextual information across interactions within applications built using the framework. In LangChain 1.1, this memory management is manual, meaning developers need to explicitly manage and maintain the chat history themselves. This feature is important for creating more coherent and contextually aware interactions in applications powered by large language models (LLMs), as it allows the system to remember previous exchanges and relevant information that can inform future responses.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# 1. Define a store for chat session\n",
        "\n",
        "# 2. New prompt with history placeholder\n",
        "# This prompt now explicitly expects the chat history as part of its messages.\n",
        "\n",
        "# 3. Define a processing step to get context from the retriever\n",
        "\n",
        "# 4. Create the RAG chain with context retrieval and history handling\n",
        "# RunnablePassthrough.assign is used to add 'context' to the input dictionary\n",
        "# before passing it to the prompt.\n",
        "\n",
        "# 5. Wrap this chain with RunnableWithMessageHistory\n",
        "# This runnable automatically manages adding messages to history\n",
        "# and retrieving them based on the session_id.\n",
        "\n",
        "# Example usage function with managed history\n"
      ],
      "metadata": {
        "id": "BEMRgewIcydS"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}